cd ~/TCM-LLM

# ========== åœæ­¢æ‰€æœ‰ ==========
pkill -9 -f llamafactory 2>/dev/null || true
pkill -9 -f torchrun 2>/dev/null || true
sleep 2

# ========== ä½¿ç”¨å·²æœ‰çš„30%æ•°æ® ==========
# å¦‚æœå·²ç»åˆ›å»ºäº† train_30p.jsonl å°±ç”¨å®ƒï¼Œå¦åˆ™åˆ›å»º
if [ ! -f data/jsonl/train_30p.jsonl ]; then
    python << 'PY'
import random
random.seed(42)
with open('data/jsonl/train.jsonl', 'r') as f:
    lines = f.readlines()
sampled = random.sample(lines, len(lines) // 3)
with open('data/jsonl/train_30p.jsonl', 'w') as f:
    f.writelines(sampled)
print(f"âœ“ åˆ›å»º30%æ•°æ®: {len(sampled):,} æ¡")
PY
fi

# ========== ä¿®å¤é…ç½®æ–‡ä»¶ ==========
cat > config/dataset_info.json << 'EOF'
{
  "tcm_train": {
    "file_name": "../data/jsonl/train.jsonl",
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations"
    }
  },
  "tcm_train_30p": {
    "file_name": "../data/jsonl/train_30p.jsonl",
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations"
    }
  }
}
EOF

# ========== åˆ›å»ºæœ€ç»ˆè®­ç»ƒè„šæœ¬ ==========
cat > train_FINAL.sh << 'SCRIPT'
#!/bin/bash
set -e

echo "=========================================="
echo "ğŸš€ TCMæ¨¡å‹è®­ç»ƒ - æœ€ç»ˆç‰ˆ"
echo "=========================================="

export CUDA_VISIBLE_DEVICES=4,5,6,7

MODEL_PATH=$(find ~/.cache/modelscope/hub/ -type d -path "*/Qwen/Qwen2___5-7B-Instruct" | grep -v temp | head -1)

echo ""
echo "é…ç½®æ£€æŸ¥:"
echo "  GPU: 4-7"
echo "  æ¨¡å‹: $MODEL_PATH"
echo "  æ•°æ®é›†: tcm_train_30p"
echo ""

mkdir -p log models/checkpoints

LOG="log/train_$(date +%Y%m%d_%H%M%S).log"
echo "ğŸ“ æ—¥å¿—: $LOG"
echo ""
echo "ğŸš€ å¼€å§‹è®­ç»ƒ..."
echo ""

llamafactory-cli train \
    --stage sft \
    --do_train \
    --model_name_or_path "$MODEL_PATH" \
    --dataset tcm_train_30p \
    --dataset_dir ./config \
    --template qwen \
    --finetuning_type lora \
    --lora_target q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \
    --lora_rank 64 \
    --lora_alpha 16 \
    --output_dir ./models/checkpoints/qwen2.5-7b-tcm-lora \
    --overwrite_output_dir \
    --cutoff_len 2048 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --learning_rate 1e-4 \
    --num_train_epochs 1.0 \
    --lr_scheduler_type cosine \
    --warmup_steps 100 \
    --logging_steps 10 \
    --save_steps 500 \
    --bf16 \
    --gradient_checkpointing \
    --quantization_bit 4 \
    --preprocessing_num_workers 16 \
    2>&1 | tee -a "$LOG"

echo ""
echo "âœ… è®­ç»ƒå®Œæˆï¼æ—¥å¿—: $LOG"
SCRIPT

chmod +x train_FINAL.sh

# ========== éªŒè¯ ==========
echo ""
echo "=========================================="
echo "éªŒè¯é…ç½®"
echo "=========================================="

python << 'PY'
import json
import os

# æ£€æŸ¥é…ç½®
with open('config/dataset_info.json', 'r') as f:
    cfg = json.load(f)

print("æ•°æ®é›†é…ç½®:")
for name, info in cfg.items():
    path = os.path.join('config', info['file_name'])
    exists = os.path.exists(path)
    if exists:
        count = sum(1 for _ in open(path))
        print(f"  âœ“ {name}: {count:,} æ¡")
    else:
        print(f"  âœ— {name}: æ–‡ä»¶ä¸å­˜åœ¨")

# æ£€æŸ¥æ˜¯å¦æœ‰tcm_train_30p
if 'tcm_train_30p' not in cfg:
    print("\nâš ï¸  é…ç½®ä¸­ç¼ºå°‘ tcm_train_30p")
else:
    print("\nâœ“ tcm_train_30p é…ç½®æ­£ç¡®")
PY

echo ""
echo "=========================================="
echo "âœ… å‡†å¤‡å®Œæˆï¼"
echo ""
echo "ç°åœ¨è¿è¡Œ:"
echo "  bash train_FINAL.sh"
echo "=========================================="