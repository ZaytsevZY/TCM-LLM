#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»è¯„æµ‹è„šæœ¬ï¼ˆæ”¯æŒå¹¶å‘ï¼‰
æ”¯æŒé›¶æ ·æœ¬å’ŒCoTä¸¤ç§æ¨¡å¼çš„è¯„æµ‹
"""
import os
import sys
import argparse
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.evaluator import ModelEvaluator, load_eval_data, save_results
from src.metrics import calculate_all_metrics, save_metrics, print_metrics
from src.prompt_builder import build_zero_shot_prompt, build_cot_prompt

# APIé…ç½®
API_CONFIG = {
    "api_key": "sk-aa792b68be91407f8ae2caf796988b7d",
    "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
    "model_name": "qwen-plus",
    "max_tokens": 4096,
    "temperature": 0.1
}

# æœ¬åœ°æ¨¡å‹é…ç½®
LOCAL_CONFIG = {
    "model_path": "Qwen/Qwen2.5-7B-Instruct",
    "lora_path": "./models/checkpoints/qwen2.5-7b-tcm-lora"
}


def main():
    parser = argparse.ArgumentParser(description='ä¸­åŒ»æ¨¡å‹è¯„æµ‹ï¼ˆæ”¯æŒå¹¶å‘ï¼‰')
    parser.add_argument('--mode', type=str, default='api', 
                        choices=['local', 'api'],
                        help='è¯„æµ‹æ¨¡å¼: local(æœ¬åœ°LoRA) æˆ– api(ä½¿ç”¨API)')
    parser.add_argument('--eval_file', type=str, default='data/evaluation/eval_100.json',
                        help='è¯„æµ‹æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='outputs/predictions',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--parallel', type=int, default=1,
                        help='å¹¶å‘æ•°ï¼ˆä»…APIæ¨¡å¼æœ‰æ•ˆï¼Œæ¨è10ï¼‰')
    parser.add_argument('--skip_zero_shot', action='store_true',
                        help='è·³è¿‡é›¶æ ·æœ¬è¯„æµ‹')
    parser.add_argument('--skip_cot', action='store_true',
                        help='è·³è¿‡CoTè¯„æµ‹')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ ä¸­åŒ»æ¨¡å‹è¯„æµ‹ç³»ç»Ÿï¼ˆå¹¶å‘ç‰ˆï¼‰")
    print("=" * 60)
    print(f"è¯„æµ‹æ¨¡å¼: {args.mode}")
    print(f"è¯„æµ‹æ–‡ä»¶: {args.eval_file}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"å¹¶å‘æ•°: {args.parallel}")
    print("")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(f"{args.output_dir}/zero_shot", exist_ok=True)
    os.makedirs(f"{args.output_dir}/cot", exist_ok=True)
    
    # åŠ è½½è¯„æµ‹æ•°æ®
    print("ğŸ“‚ åŠ è½½è¯„æµ‹æ•°æ®...")
    eval_data = load_eval_data(args.eval_file)
    print(f"âœ“ å·²åŠ è½½ {len(eval_data)} æ¡è¯„æµ‹æ•°æ®\n")
    
    # åˆå§‹åŒ–è¯„æµ‹å™¨
    if args.mode == "local":
        evaluator = ModelEvaluator(
            mode="local",
            model_path=LOCAL_CONFIG["model_path"],
            lora_path=LOCAL_CONFIG["lora_path"]
        )
        # æœ¬åœ°æ¨¡å¼å¼ºåˆ¶å•çº¿ç¨‹
        num_workers = 1
        print("âš ï¸  æœ¬åœ°æ¨¡å¼ä»…æ”¯æŒå•çº¿ç¨‹è¯„æµ‹")
    else:
        evaluator = ModelEvaluator(
            mode="api",
            api_config=API_CONFIG
        )
        num_workers = args.parallel
    
    # ========================================
    # é›¶æ ·æœ¬è¯„æµ‹
    # ========================================
    if not args.skip_zero_shot:
        print("\n" + "=" * 60)
        print("1ï¸âƒ£ é›¶æ ·æœ¬è¯„æµ‹")
        print("=" * 60)
        
        zero_shot_results = evaluator.batch_evaluate(
            eval_data=eval_data,
            prompt_builder=build_zero_shot_prompt,
            mode_name="é›¶æ ·æœ¬",
            max_tokens=2048,
            num_workers=num_workers
        )
        
        # ä¿å­˜ç»“æœ
        save_results(
            zero_shot_results,
            f"{args.output_dir}/zero_shot/predictions.json"
        )
        
        # è®¡ç®—æŒ‡æ ‡
        zero_shot_metrics = calculate_all_metrics(zero_shot_results)
        save_metrics(
            zero_shot_metrics,
            f"{args.output_dir}/zero_shot/metrics.json"
        )
        print_metrics(zero_shot_metrics, "é›¶æ ·æœ¬è¯„æµ‹ç»“æœ")
    
    # ========================================
    # CoTè¯„æµ‹
    # ========================================
    if not args.skip_cot:
        print("\n" + "=" * 60)
        print("2ï¸âƒ£ CoTè¯„æµ‹")
        print("=" * 60)
        
        cot_results = evaluator.batch_evaluate(
            eval_data=eval_data,
            prompt_builder=build_cot_prompt,
            mode_name="CoT",
            max_tokens=4096,
            num_workers=num_workers
        )
        
        # ä¿å­˜ç»“æœ
        save_results(
            cot_results,
            f"{args.output_dir}/cot/predictions.json"
        )
        
        # è®¡ç®—æŒ‡æ ‡
        cot_metrics = calculate_all_metrics(cot_results)
        save_metrics(
            cot_metrics,
            f"{args.output_dir}/cot/metrics.json"
        )
        print_metrics(cot_metrics, "CoTè¯„æµ‹ç»“æœ")
    
    # ========================================
    # å¯¹æ¯”åˆ†æ
    # ========================================
    if not args.skip_zero_shot and not args.skip_cot:
        print("\n" + "=" * 60)
        print("ğŸ“Š å¯¹æ¯”åˆ†æ")
        print("=" * 60)
        
        print(f"\n{'æŒ‡æ ‡':<20} {'é›¶æ ·æœ¬':>15} {'CoT':>15} {'æå‡':>15}")
        print("-" * 70)
        
        em_diff = cot_metrics['exact_match'] - zero_shot_metrics['exact_match']
        print(f"{'ç²¾ç¡®åŒ¹é…ç‡':<20} {zero_shot_metrics['exact_match']:>14.2%} "
              f"{cot_metrics['exact_match']:>14.2%} {em_diff:>+14.2%}")
        
        f1_diff = cot_metrics['avg_f1'] - zero_shot_metrics['avg_f1']
        print(f"{'å¹³å‡F1':<20} {zero_shot_metrics['avg_f1']:>14.4f} "
              f"{cot_metrics['avg_f1']:>14.4f} {f1_diff:>+14.4f}")
        
        rouge_l_diff = (cot_metrics['rouge_scores']['rouge-l'] - 
                        zero_shot_metrics['rouge_scores']['rouge-l'])
        print(f"{'ROUGE-L':<20} {zero_shot_metrics['rouge_scores']['rouge-l']:>14.4f} "
              f"{cot_metrics['rouge_scores']['rouge-l']:>14.4f} {rouge_l_diff:>+14.4f}")
        
        time_diff = cot_metrics['avg_inference_time'] - zero_shot_metrics['avg_inference_time']
        print(f"{'å¹³å‡æ¨ç†æ—¶é—´(ç§’)':<20} {zero_shot_metrics['avg_inference_time']:>14.2f} "
              f"{cot_metrics['avg_inference_time']:>14.2f} {time_diff:>+14.2f}")
        
        print("-" * 70)
    
    print("\n" + "=" * 60)
    print("âœ… è¯„æµ‹å®Œæˆï¼")
    print("=" * 60)
    print(f"\nç»“æœä¿å­˜åœ¨: {args.output_dir}/")
    print("\nä¸‹ä¸€æ­¥:")
    print("  1. æŸ¥çœ‹è¯¦ç»†ç»“æœ: cat outputs/predictions/zero_shot/metrics.json")
    print("  2. è¿è¡Œå®Œæ•´è¯„æµ‹: python scripts/05_evaluate.py --eval_file data/evaluation/eval_500.json --parallel 10")
    print("  3. ç”Ÿæˆåˆ†ææŠ¥å‘Š: python scripts/06_analyze_results.py")


if __name__ == "__main__":
    main()
