#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´å¯¹æ¯”å®éªŒï¼ˆCoTç­”æ¡ˆæå–ç‰ˆï¼‰
"""
import os
import sys
import argparse
import json
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.evaluator import ModelEvaluator, load_eval_data, save_results
from src.metrics import calculate_all_metrics, save_metrics, print_metrics
from src.prompt_builder import build_zero_shot_prompt, build_cot_prompt

# APIé…ç½®
API_CONFIG = {
    "api_key": "sk-aa792b68be91407f8ae2caf796988b7d",
    "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
    "model_name": "qwen-plus",
    "max_tokens": 4096,
    "temperature": 0.1
}

# æœ¬åœ°LoRAé…ç½®
LOCAL_CONFIG = {
    "model_path": "/home/zhayi/.cache/modelscope/hub/models/Qwen/Qwen2___5-7B-Instruct",
    "lora_path": "./models/checkpoints/qwen2.5-7b-tcm-lora"
}


def run_experiment(
    evaluator,
    eval_data,
    experiment_name,
    output_base_dir,
    num_workers=10
):
    """è¿è¡Œå•ä¸ªå®éªŒï¼ˆé›¶æ ·æœ¬+CoTï¼‰"""
    print("\n" + "=" * 70)
    print(f"ğŸ§ª å®éªŒ: {experiment_name}")
    print("=" * 70)
    
    exp_dir = f"{output_base_dir}/{experiment_name}"
    os.makedirs(f"{exp_dir}/zero_shot", exist_ok=True)
    os.makedirs(f"{exp_dir}/cot", exist_ok=True)
    
    results = {}
    
    # 1. é›¶æ ·æœ¬è¯„æµ‹
    print(f"\n[{experiment_name}] 1/2 é›¶æ ·æœ¬è¯„æµ‹...")
    zero_shot_results = evaluator.batch_evaluate(
        eval_data=eval_data,
        prompt_builder=build_zero_shot_prompt,
        mode_name=f"{experiment_name}-é›¶æ ·æœ¬",
        max_tokens=2048,
        num_workers=num_workers,
        is_cot=False  # é›¶æ ·æœ¬æ¨¡å¼
    )
    
    save_results(zero_shot_results, f"{exp_dir}/zero_shot/predictions.json")
    zero_shot_metrics = calculate_all_metrics(zero_shot_results)
    save_metrics(zero_shot_metrics, f"{exp_dir}/zero_shot/metrics.json")
    print_metrics(zero_shot_metrics, f"{experiment_name} - é›¶æ ·æœ¬")
    
    results['zero_shot'] = {
        'predictions': zero_shot_results,
        'metrics': zero_shot_metrics
    }
    
    # 2. CoTè¯„æµ‹ï¼ˆæå–ç­”æ¡ˆæ ‡ç­¾ï¼‰
    print(f"\n[{experiment_name}] 2/2 CoTè¯„æµ‹ï¼ˆç­”æ¡ˆæå–æ¨¡å¼ï¼‰...")
    cot_results = evaluator.batch_evaluate(
        eval_data=eval_data,
        prompt_builder=build_cot_prompt,
        mode_name=f"{experiment_name}-CoT",
        max_tokens=4096,
        num_workers=num_workers,
        is_cot=True  # âœ¨ CoTæ¨¡å¼ï¼Œä¼šæå–<ç­”æ¡ˆ>æ ‡ç­¾
    )
    
    save_results(cot_results, f"{exp_dir}/cot/predictions.json")
    cot_metrics = calculate_all_metrics(cot_results)
    save_metrics(cot_metrics, f"{exp_dir}/cot/metrics.json")
    print_metrics(cot_metrics, f"{experiment_name} - CoT")
    
    results['cot'] = {
        'predictions': cot_results,
        'metrics': cot_metrics
    }
    
    return results


def print_final_comparison(all_results):
    """æ‰“å°æœ€ç»ˆå¯¹æ¯”"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æœ€ç»ˆå¯¹æ¯”åˆ†æ")
    print("=" * 80)
    
    api_zs = all_results['api_baseline']['zero_shot']['metrics']
    api_cot = all_results['api_baseline']['cot']['metrics']
    lora_zs = all_results['lora_finetuned']['zero_shot']['metrics']
    lora_cot = all_results['lora_finetuned']['cot']['metrics']
    
    print(f"\n{'å®éªŒç»„':<25} {'ç²¾ç¡®åŒ¹é…ç‡':>12} {'å¹³å‡F1':>12} {'ROUGE-L':>12} {'æ¨ç†æ—¶é—´':>12}")
    print("-" * 80)
    
    print(f"{'APIåŸºçº¿-é›¶æ ·æœ¬':<25} {api_zs['exact_match']:>11.2%} "
          f"{api_zs['avg_f1']:>11.4f} {api_zs['rouge_scores']['rouge-l']:>11.4f} "
          f"{api_zs['avg_inference_time']:>11.2f}s")
    
    print(f"{'APIåŸºçº¿-CoT(æå–)':<25} {api_cot['exact_match']:>11.2%} "
          f"{api_cot['avg_f1']:>11.4f} {api_cot['rouge_scores']['rouge-l']:>11.4f} "
          f"{api_cot['avg_inference_time']:>11.2f}s")
    
    print(f"{'LoRAå¾®è°ƒ-é›¶æ ·æœ¬':<25} {lora_zs['exact_match']:>11.2%} "
          f"{lora_zs['avg_f1']:>11.4f} {lora_zs['rouge_scores']['rouge-l']:>11.4f} "
          f"{lora_zs['avg_inference_time']:>11.2f}s")
    
    print(f"{'LoRAå¾®è°ƒ-CoT(æå–)':<25} {lora_cot['exact_match']:>11.2%} "
          f"{lora_cot['avg_f1']:>11.4f} {lora_cot['rouge_scores']['rouge-l']:>11.4f} "
          f"{lora_cot['avg_inference_time']:>11.2f}s")
    
    print("-" * 80)
    
    # å…³é”®å¯¹æ¯”
    print("\nå…³é”®å‘ç°:")
    
    # 1. å¾®è°ƒæ•ˆæœ
    ft_improvement = lora_zs['avg_f1'] - api_zs['avg_f1']
    print(f"1. å¾®è°ƒæ•ˆæœ: F1æå‡ {ft_improvement:+.4f} ({ft_improvement/api_zs['avg_f1']*100:+.1f}%)")
    
    # 2. CoTæ•ˆæœï¼ˆAPIï¼‰
    cot_improvement_api = api_cot['avg_f1'] - api_zs['avg_f1']
    print(f"2. CoTæ•ˆæœ-API: F1å˜åŒ– {cot_improvement_api:+.4f} ({cot_improvement_api/api_zs['avg_f1']*100:+.1f}%)")
    
    # 3. CoTæ•ˆæœï¼ˆLoRAï¼‰
    cot_improvement_lora = lora_cot['avg_f1'] - lora_zs['avg_f1']
    print(f"3. CoTæ•ˆæœ-LoRA: F1å˜åŒ– {cot_improvement_lora:+.4f} ({cot_improvement_lora/lora_zs['avg_f1']*100:+.1f}%)")
    
    # 4. æœ€ä½³ç»„åˆ
    best_f1 = max(api_zs['avg_f1'], api_cot['avg_f1'], 
                  lora_zs['avg_f1'], lora_cot['avg_f1'])
    best_group = ""
    if best_f1 == lora_cot['avg_f1']:
        best_group = "LoRAå¾®è°ƒ+CoT"
    elif best_f1 == lora_zs['avg_f1']:
        best_group = "LoRAå¾®è°ƒ+é›¶æ ·æœ¬"
    elif best_f1 == api_cot['avg_f1']:
        best_group = "APIåŸºçº¿+CoT"
    else:
        best_group = "APIåŸºçº¿+é›¶æ ·æœ¬"
    
    print(f"4. æœ€ä½³ç»„åˆ: {best_group} (F1={best_f1:.4f})")
    
    print("=" * 80)


def main():
    parser = argparse.ArgumentParser(description='å®Œæ•´å¯¹æ¯”å®éªŒï¼ˆCoTç­”æ¡ˆæå–ç‰ˆï¼‰')
    parser.add_argument('--eval_file', type=str, default='data/evaluation/eval_100.json')
    parser.add_argument('--output_dir', type=str, default='outputs/comparison_v2')
    parser.add_argument('--parallel', type=int, default=10)
    parser.add_argument('--skip_api', action='store_true')
    parser.add_argument('--skip_lora', action='store_true')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸš€ å®Œæ•´å¯¹æ¯”å®éªŒç³»ç»Ÿ v2ï¼ˆCoTç­”æ¡ˆæå–ï¼‰")
    print("=" * 80)
    print(f"è¯„æµ‹æ–‡ä»¶: {args.eval_file}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"å¹¶å‘æ•°: {args.parallel}")
    print("\nâœ¨ æ–°åŠŸèƒ½: CoTä¼šè¦æ±‚æ¨¡å‹è¾“å‡º<ç­”æ¡ˆ>æ ‡ç­¾ï¼Œåªæå–æ ‡ç­¾å†…å®¹è¿›è¡Œè¯„æµ‹")
    print("")
    
    # åŠ è½½è¯„æµ‹æ•°æ®
    print("ğŸ“‚ åŠ è½½è¯„æµ‹æ•°æ®...")
    eval_data = load_eval_data(args.eval_file)
    print(f"âœ“ å·²åŠ è½½ {len(eval_data)} æ¡æ•°æ®\n")
    
    all_results = {}
    
    # å®éªŒ1: APIåŸºçº¿
    if not args.skip_api:
        api_evaluator = ModelEvaluator(mode="api", api_config=API_CONFIG)
        all_results['api_baseline'] = run_experiment(
            evaluator=api_evaluator,
            eval_data=eval_data,
            experiment_name="api_baseline",
            output_base_dir=args.output_dir,
            num_workers=args.parallel
        )
    
    # å®éªŒ2: LoRAå¾®è°ƒ
    if not args.skip_lora:
        lora_evaluator = ModelEvaluator(
            mode="local",
            model_path=LOCAL_CONFIG["model_path"],
            lora_path=LOCAL_CONFIG["lora_path"]
        )
        all_results['lora_finetuned'] = run_experiment(
            evaluator=lora_evaluator,
            eval_data=eval_data,
            experiment_name="lora_finetuned",
            output_base_dir=args.output_dir,
            num_workers=1
        )
    
    # æœ€ç»ˆå¯¹æ¯”
    if not args.skip_api and not args.skip_lora:
        print_final_comparison(all_results)
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    summary = {
        'eval_file': args.eval_file,
        'total_samples': len(eval_data),
        'experiments': {}
    }
    
    for exp_name, exp_results in all_results.items():
        summary['experiments'][exp_name] = {
            'zero_shot': exp_results['zero_shot']['metrics'],
            'cot': exp_results['cot']['metrics']
        }
    
    with open(f"{args.output_dir}/summary.json", 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å®Œæ•´ç»“æœå·²ä¿å­˜: {args.output_dir}/summary.json")


if __name__ == "__main__":
    main()
