#!/usr/bin/env python3
"""æµ‹è¯•è®­ç»ƒé…ç½®æ˜¯å¦æ­£ç¡®"""
import os
import json
from datasets import load_from_disk

print("ğŸ” æµ‹è¯•è®­ç»ƒé…ç½®...\n")

# 1. æ£€æŸ¥æ•°æ®é›†é…ç½®
print("[1] æ£€æŸ¥dataset_info.json...")
with open("config/dataset_info.json", 'r') as f:
    dataset_info = json.load(f)
    print(f"  âœ“ é…ç½®æ–‡ä»¶è¯»å–æˆåŠŸ")
    print(f"  âœ“ è®­ç»ƒé›†é…ç½®: {list(dataset_info.keys())}")

# 2. æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
print("\n[2] æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶...")
for name, info in dataset_info.items():
    path = info['file_name'].replace('../', '')
    if os.path.exists(path):
        ds = load_from_disk(path)
        print(f"  âœ“ {name}: {len(ds):,} æ¡")
    else:
        print(f"  âœ— {name}: æœªæ‰¾åˆ° ({path})")

# 3. æ£€æŸ¥æ•°æ®æ ¼å¼
print("\n[3] æ£€æŸ¥æ•°æ®æ ¼å¼...")
train = load_from_disk("data/processed/train")
sample = train[0]
required = ['instruction', 'input', 'output']
has_all = all(field in sample for field in required)
print(f"  å­—æ®µ: {list(sample.keys())}")
print(f"  æ ¼å¼æ£€æŸ¥: {'âœ“ æ­£ç¡®' if has_all else 'âœ— ç¼ºå°‘å­—æ®µ'}")

# 4. æ£€æŸ¥ç›®å½•
print("\n[4] æ£€æŸ¥è¾“å‡ºç›®å½•...")
dirs = ['models/checkpoints', 'outputs/logs']
for d in dirs:
    if os.path.exists(d):
        print(f"  âœ“ {d}")
    else:
        os.makedirs(d)
        print(f"  âœ“ {d} (å·²åˆ›å»º)")

print("\n" + "="*60)
print("âœ… é…ç½®æ£€æŸ¥å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
print("\nä¸‹ä¸€æ­¥:")
print("  bash scripts/04_train.sh")
