#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬åœ°LoRAæ¨¡å‹å¿«é€Ÿæµ‹è¯•è„šæœ¬
æµ‹è¯•æ¨¡å‹åŠ è½½ã€æ¨ç†ã€è¾“å‡ºæ ¼å¼ç­‰åŸºæœ¬åŠŸèƒ½
"""
import os
import sys
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# é…ç½®
BASE_MODEL_PATH = "/home/zhayi/.cache/modelscope/hub/Qwen/Qwen2___5-7B-Instruct"
LORA_PATH = "./models/checkpoints/qwen2.5-7b-tcm-lora"

# æµ‹è¯•æ ·ä¾‹
TEST_CASES = [
    {
        "question": "æ‚£è€…ä¸»è¯‰ï¼šå’³å—½ã€å’³ç—°1å‘¨ï¼ŒåŠ é‡3å¤©ã€‚ç°ç—‡ï¼šå’³å—½é¢‘ç¹ï¼Œç—°è‰²é»„ç¨ ï¼Œéš¾ä»¥å’³å‡ºï¼Œä¼´æœ‰å‘çƒ­ï¼Œä½“æ¸©38.5â„ƒã€‚èˆŒè´¨çº¢ï¼Œè‹”é»„è…»ï¼Œè„‰æ»‘æ•°ã€‚è¯·é—®åº”è¯Šæ–­ä¸ºä½•è¯å‹ï¼Ÿ",
        "expected_pattern": "ç—°çƒ­å£…è‚º",
        "category": "è¾¨è¯"
    },
    {
        "question": "æ‚£è€…å¥³æ€§ï¼Œ35å²ï¼Œæœˆç»ä¸è°ƒï¼Œç»æœŸå»¶åï¼Œé‡å°‘è‰²æ·¡ï¼Œä¼´æœ‰è…°è†é…¸è½¯ï¼Œå¤´æ™•è€³é¸£ã€‚èˆŒæ·¡è‹”ç™½ï¼Œè„‰æ²‰ç»†ã€‚è¯·å¼€å…·å¤„æ–¹ã€‚",
        "expected_keywords": ["ç†Ÿåœ°é»„", "å½“å½’", "ç™½èŠ"],
        "category": "å¼€æ–¹"
    },
    {
        "question": "éº»é»„çš„åŠŸæ•ˆæ˜¯ä»€ä¹ˆï¼Ÿ",
        "expected_keywords": ["å‘æ±—", "å¹³å–˜", "åˆ©æ°´"],
        "category": "è¯æ€§"
    }
]


def print_separator(char="=", length=80):
    """æ‰“å°åˆ†éš”çº¿"""
    print(char * length)


def print_section(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print_separator()
    print(f"ğŸ” {title}")
    print_separator()


def load_model():
    """åŠ è½½æ¨¡å‹å’ŒLoRAæƒé‡"""
    print_section("æ­¥éª¤ 1/4: åŠ è½½æ¨¡å‹")
    
    try:
        print(f"ğŸ“‚ åŸºç¡€æ¨¡å‹è·¯å¾„: {BASE_MODEL_PATH}")
        print(f"ğŸ“‚ LoRAæƒé‡è·¯å¾„: {LORA_PATH}")
        print()
        
        # æ£€æŸ¥è·¯å¾„
        if not os.path.exists(BASE_MODEL_PATH):
            print(f"âŒ åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {BASE_MODEL_PATH}")
            print("ğŸ’¡ è¯·ä¿®æ”¹è„šæœ¬ä¸­çš„ BASE_MODEL_PATH")
            sys.exit(1)
            
        if not os.path.exists(LORA_PATH):
            print(f"âŒ LoRAæƒé‡è·¯å¾„ä¸å­˜åœ¨: {LORA_PATH}")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”ŸæˆLoRAæƒé‡")
            sys.exit(1)
        
        # åŠ è½½tokenizer
        print("â³ åŠ è½½ tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            BASE_MODEL_PATH,
            trust_remote_code=True
        )
        print("âœ“ Tokenizer åŠ è½½æˆåŠŸ")
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        print("â³ åŠ è½½åŸºç¡€æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        print("âœ“ åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åŠ è½½LoRAæƒé‡
        print("â³ åŠ è½½ LoRA æƒé‡...")
        model = PeftModel.from_pretrained(base_model, LORA_PATH)
        model = model.merge_and_unload()  # åˆå¹¶æƒé‡ä»¥æé«˜æ¨ç†é€Ÿåº¦
        print("âœ“ LoRA æƒé‡åŠ è½½æˆåŠŸ")
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print()
        print("ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"  - è®¾å¤‡: {next(model.parameters()).device}")
        print(f"  - æ•°æ®ç±»å‹: {next(model.parameters()).dtype}")
        print(f"  - å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def test_basic_generation(model, tokenizer):
    """æµ‹è¯•åŸºæœ¬ç”Ÿæˆèƒ½åŠ›"""
    print_section("æ­¥éª¤ 2/4: åŸºæœ¬ç”Ÿæˆæµ‹è¯•")
    
    test_prompt = "ä½ å¥½"
    
    print(f"è¾“å…¥: {test_prompt}")
    print()
    
    try:
        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "user", "content": test_prompt}]
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # ç¼–ç 
        inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        # ç”Ÿæˆ
        print("â³ ç”Ÿæˆä¸­...")
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.1,
                do_sample=True,
                top_p=0.9
            )
        
        inference_time = time.time() - start_time
        
        # è§£ç 
        response = tokenizer.decode(
            outputs[0][len(inputs.input_ids[0]):],
            skip_special_tokens=True
        )
        
        print(f"è¾“å‡º: {response}")
        print(f"â±ï¸  æ¨ç†æ—¶é—´: {inference_time:.2f}ç§’")
        print("âœ“ åŸºæœ¬ç”Ÿæˆæµ‹è¯•é€šè¿‡")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬ç”Ÿæˆæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_tcm_cases(model, tokenizer):
    """æµ‹è¯•ä¸­åŒ»æ¡ˆä¾‹"""
    print_section("æ­¥éª¤ 3/4: ä¸­åŒ»æ¡ˆä¾‹æµ‹è¯•")
    
    passed = 0
    failed = 0
    
    for i, case in enumerate(TEST_CASES, 1):
        print(f"\n{'â”€' * 80}")
        print(f"ğŸ“‹ æ¡ˆä¾‹ {i}/{len(TEST_CASES)}: {case['category']}")
        print(f"{'â”€' * 80}")
        
        print(f"é—®é¢˜:\n{case['question']}\n")
        
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [{"role": "user", "content": case['question']}]
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ç¼–ç 
            inputs = tokenizer([text], return_tensors="pt").to(model.device)
            
            # ç”Ÿæˆ
            print("â³ ç”Ÿæˆä¸­...")
            start_time = time.time()
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.1,
                    do_sample=True,
                    top_p=0.9
                )
            
            inference_time = time.time() - start_time
            
            # è§£ç 
            response = tokenizer.decode(
                outputs[0][len(inputs.input_ids[0]):],
                skip_special_tokens=True
            )
            
            print(f"å›ç­”:\n{response}\n")
            print(f"â±ï¸  æ¨ç†æ—¶é—´: {inference_time:.2f}ç§’")
            
            # ç®€å•éªŒè¯
            check_passed = True
            
            if 'expected_pattern' in case:
                if case['expected_pattern'] in response:
                    print(f"âœ“ åŒ…å«é¢„æœŸè¯å‹: {case['expected_pattern']}")
                else:
                    print(f"âš ï¸  æœªæ‰¾åˆ°é¢„æœŸè¯å‹: {case['expected_pattern']}")
                    check_passed = False
            
            if 'expected_keywords' in case:
                found_keywords = [kw for kw in case['expected_keywords'] if kw in response]
                if found_keywords:
                    print(f"âœ“ åŒ…å«å…³é”®è¯: {', '.join(found_keywords)}")
                else:
                    print(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½•é¢„æœŸå…³é”®è¯: {', '.join(case['expected_keywords'])}")
                    check_passed = False
            
            if check_passed:
                print("âœ“ æ¡ˆä¾‹æµ‹è¯•é€šè¿‡")
                passed += 1
            else:
                print("âš ï¸  æ¡ˆä¾‹æµ‹è¯•éƒ¨åˆ†é€šè¿‡")
                failed += 1
                
        except Exception as e:
            print(f"âŒ æ¡ˆä¾‹æµ‹è¯•å¤±è´¥: {str(e)}")
            failed += 1
            import traceback
            traceback.print_exc()
    
    print(f"\n{'â”€' * 80}")
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{len(TEST_CASES)} é€šè¿‡")
    print(f"{'â”€' * 80}")
    
    return passed, failed


def test_performance(model, tokenizer):
    """æ€§èƒ½æµ‹è¯•"""
    print_section("æ­¥éª¤ 4/4: æ€§èƒ½æµ‹è¯•")
    
    test_prompt = "æ‚£è€…å‡ºç°å¤´ç—›ã€å‘çƒ­ç—‡çŠ¶ï¼Œè¯·è¿›è¡Œè¾¨è¯åˆ†æã€‚"
    num_runs = 5
    
    print(f"æµ‹è¯•æç¤º: {test_prompt}")
    print(f"è¿è¡Œæ¬¡æ•°: {num_runs}")
    print()
    
    times = []
    
    try:
        for i in range(num_runs):
            # æ„å»ºæ¶ˆæ¯
            messages = [{"role": "user", "content": test_prompt}]
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = tokenizer([text], return_tensors="pt").to(model.device)
            
            start_time = time.time()
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=256,
                    temperature=0.1,
                    do_sample=True
                )
            
            inference_time = time.time() - start_time
            times.append(inference_time)
            
            print(f"  ç¬¬ {i+1} æ¬¡: {inference_time:.2f}ç§’")
        
        print()
        print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"  - å¹³å‡æ—¶é—´: {sum(times)/len(times):.2f}ç§’")
        print(f"  - æœ€å¿«: {min(times):.2f}ç§’")
        print(f"  - æœ€æ…¢: {max(times):.2f}ç§’")
        print("âœ“ æ€§èƒ½æµ‹è¯•å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print_separator("=")
    print("ğŸš€ æœ¬åœ°LoRAæ¨¡å‹æµ‹è¯•ç³»ç»Ÿ")
    print_separator("=")
    print()
    
    # æ­¥éª¤1: åŠ è½½æ¨¡å‹
    model, tokenizer = load_model()
    print()
    
    # æ­¥éª¤2: åŸºæœ¬ç”Ÿæˆæµ‹è¯•
    if not test_basic_generation(model, tokenizer):
        print("\nâŒ åŸºæœ¬ç”Ÿæˆæµ‹è¯•å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        sys.exit(1)
    print()
    
    # æ­¥éª¤3: ä¸­åŒ»æ¡ˆä¾‹æµ‹è¯•
    passed, failed = test_tcm_cases(model, tokenizer)
    print()
    
    # æ­¥éª¤4: æ€§èƒ½æµ‹è¯•
    test_performance(model, tokenizer)
    print()
    
    # æœ€ç»ˆæ€»ç»“
    print_separator("=")
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
    print_separator("=")
    print(f"âœ“ æ¨¡å‹åŠ è½½: æˆåŠŸ")
    print(f"âœ“ åŸºæœ¬ç”Ÿæˆ: æˆåŠŸ")
    print(f"âœ“ ä¸­åŒ»æ¡ˆä¾‹: {passed}/{len(TEST_CASES)} é€šè¿‡")
    print(f"âœ“ æ€§èƒ½æµ‹è¯•: å®Œæˆ")
    print()
    
    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å·¥ä½œæ­£å¸¸")
        print("\nä¸‹ä¸€æ­¥:")
        print("  python scripts/07_full_comparison.py --eval_file data/evaluation/eval_100.json")
    else:
        print(f"âš ï¸  {failed} ä¸ªæ¡ˆä¾‹æœªå®Œå…¨é€šè¿‡ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹è¾“å‡ºè´¨é‡")
        print("\nå¯èƒ½åŸå› :")
        print("  1. è®­ç»ƒæ•°æ®ä¸è¶³")
        print("  2. è®­ç»ƒè½®æ•°ä¸å¤Ÿ")
        print("  3. å­¦ä¹ ç‡è®¾ç½®ä¸å½“")
    
    print_separator("=")


if __name__ == "__main__":
    main()