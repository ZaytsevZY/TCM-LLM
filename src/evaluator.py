#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯„æµ‹å·¥å…·ç±»ï¼ˆæ”¯æŒå¹¶å‘ + CoTç­”æ¡ˆæå–ï¼‰
"""
import json
import time
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import openai
from tqdm import tqdm

class ModelEvaluator:
    """æ¨¡å‹è¯„æµ‹å™¨"""
    
    def __init__(self, mode="local", model_path=None, lora_path=None, api_config=None):
        """åˆå§‹åŒ–è¯„æµ‹å™¨"""
        self.mode = mode
        
        if mode == "local":
            print("ğŸ”§ åŠ è½½æœ¬åœ°LoRAæ¨¡å‹...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            # åŠ è½½åŸºåº§æ¨¡å‹
            self.base_model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # åŠ è½½LoRAæƒé‡
            self.model = PeftModel.from_pretrained(
                self.base_model,
                lora_path,
                torch_dtype=torch.bfloat16
            )
            self.model.eval()
            print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
            
        elif mode == "api":
            print("ğŸ”§ é…ç½®APIè¯„æµ‹...")
            self.client = openai.OpenAI(
                api_key=api_config["api_key"],
                base_url=api_config["base_url"]
            )
            self.api_model = api_config["model_name"]
            self.api_config = api_config
            print("âœ“ APIé…ç½®å®Œæˆ")
    
    def generate(self, prompt: str, max_tokens: int = 2048, temperature: float = 0.1) -> str:
        """ç”Ÿæˆå›ç­”"""
        if self.mode == "local":
            return self._generate_local(prompt, max_tokens, temperature)
        elif self.mode == "api":
            return self._generate_api(prompt, max_tokens, temperature)
    
    def _generate_local(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """æœ¬åœ°æ¨¡å‹ç”Ÿæˆ"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=temperature > 0,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        generated_text = self.tokenizer.decode(
            outputs[0][len(inputs.input_ids[0]):],
            skip_special_tokens=True
        )
        return generated_text.strip()
    
    def _generate_api(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """APIç”Ÿæˆï¼ˆå¸¦é‡è¯•ï¼‰"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.api_model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                else:
                    print(f"APIè°ƒç”¨å¤±è´¥: {e}")
                    return ""
    
    def _evaluate_single(
        self,
        item: Dict[str, Any],
        prompt_builder,
        max_tokens: int,
        is_cot: bool = False
    ) -> Dict[str, Any]:
        """
        è¯„æµ‹å•ä¸ªæ ·æœ¬
        
        Args:
            item: å•ä¸ªè¯„æµ‹æ ·æœ¬
            prompt_builder: promptæ„å»ºå‡½æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            is_cot: æ˜¯å¦æ˜¯CoTæ¨¡å¼ï¼ˆéœ€è¦æå–ç­”æ¡ˆï¼‰
            
        Returns:
            è¯„æµ‹ç»“æœ
        """
        start_time = time.time()
        
        # æ„å»ºprompt
        prompt = prompt_builder(item["full_question"])
        
        # ç”Ÿæˆå›ç­”
        raw_prediction = self.generate(prompt, max_tokens=max_tokens)
        
        inference_time = time.time() - start_time
        
        # å¦‚æœæ˜¯CoTï¼Œæå–ç­”æ¡ˆæ ‡ç­¾
        if is_cot:
            from src.prompt_builder import extract_answer_from_cot
            extracted_answer, has_tags = extract_answer_from_cot(raw_prediction)
            
            result = {
                "id": item["id"],
                "instruction": item["instruction"],
                "input": item["input"],
                "full_question": item["full_question"],
                "reference": item["output"],
                "raw_prediction": raw_prediction,  # ä¿å­˜å®Œæ•´è¾“å‡º
                "prediction": extracted_answer,     # ç”¨äºè¯„æµ‹çš„ç­”æ¡ˆ
                "has_answer_tags": has_tags,        # æ˜¯å¦åŒ…å«æ ‡ç­¾
                "inference_time": inference_time
            }
        else:
            # é›¶æ ·æœ¬æ¨¡å¼ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹è¾“å‡º
            result = {
                "id": item["id"],
                "instruction": item["instruction"],
                "input": item["input"],
                "full_question": item["full_question"],
                "reference": item["output"],
                "prediction": raw_prediction,
                "inference_time": inference_time
            }
        
        return result
    
    def batch_evaluate(
        self,
        eval_data: List[Dict[str, Any]],
        prompt_builder,
        mode_name: str,
        max_tokens: int = 2048,
        num_workers: int = 1,
        is_cot: bool = False
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡è¯„æµ‹ï¼ˆæ”¯æŒå¹¶å‘ï¼‰
        
        Args:
            eval_data: è¯„æµ‹æ•°æ®åˆ—è¡¨
            prompt_builder: promptæ„å»ºå‡½æ•°
            mode_name: è¯„æµ‹æ¨¡å¼åç§°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            num_workers: å¹¶å‘çº¿ç¨‹æ•°
            is_cot: æ˜¯å¦æ˜¯CoTæ¨¡å¼
            
        Returns:
            è¯„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        
        print(f"\nğŸ”„ å¼€å§‹{mode_name}è¯„æµ‹ ({len(eval_data)}æ¡)...")
        print(f"å¹¶å‘æ•°: {num_workers}")
        if is_cot:
            print("âš ï¸  CoTæ¨¡å¼ï¼šå°†æå–<ç­”æ¡ˆ>æ ‡ç­¾ä¸­çš„å†…å®¹è¿›è¡Œè¯„æµ‹")
        
        if self.mode == "api" and num_workers > 1:
            results = self._batch_evaluate_parallel(
                eval_data, prompt_builder, mode_name, max_tokens, num_workers, is_cot
            )
        else:
            results = self._batch_evaluate_sequential(
                eval_data, prompt_builder, mode_name, max_tokens, is_cot
            )
        
        # ç»Ÿè®¡CoTæ ‡ç­¾ä½¿ç”¨æƒ…å†µ
        if is_cot:
            has_tags_count = sum(1 for r in results if r.get('has_answer_tags', False))
            print(f"âœ“ {mode_name}è¯„æµ‹å®Œæˆ - {has_tags_count}/{len(results)} æ¡ä½¿ç”¨äº†ç­”æ¡ˆæ ‡ç­¾")
        else:
            print(f"âœ“ {mode_name}è¯„æµ‹å®Œæˆ")
        
        return results
    
    def _batch_evaluate_sequential(
        self,
        eval_data: List[Dict[str, Any]],
        prompt_builder,
        mode_name: str,
        max_tokens: int,
        is_cot: bool
    ) -> List[Dict[str, Any]]:
        """ä¸²è¡Œè¯„æµ‹"""
        results = []
        for item in tqdm(eval_data, desc=f"{mode_name}è¯„æµ‹"):
            result = self._evaluate_single(item, prompt_builder, max_tokens, is_cot)
            results.append(result)
        return results
    
    def _batch_evaluate_parallel(
        self,
        eval_data: List[Dict[str, Any]],
        prompt_builder,
        mode_name: str,
        max_tokens: int,
        num_workers: int,
        is_cot: bool
    ) -> List[Dict[str, Any]]:
        """å¹¶è¡Œè¯„æµ‹ï¼ˆAPIæ¨¡å¼ï¼‰"""
        results = [None] * len(eval_data)
        
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            future_to_idx = {
                executor.submit(
                    self._evaluate_single, 
                    item, 
                    prompt_builder, 
                    max_tokens,
                    is_cot
                ): idx
                for idx, item in enumerate(eval_data)
            }
            
            with tqdm(total=len(eval_data), desc=f"{mode_name}è¯„æµ‹(å¹¶å‘)") as pbar:
                for future in as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    try:
                        result = future.result()
                        results[idx] = result
                    except Exception as e:
                        print(f"\næ ·æœ¬ {idx} è¯„æµ‹å¤±è´¥: {e}")
                        results[idx] = {
                            "id": eval_data[idx]["id"],
                            "prediction": "",
                            "inference_time": 0,
                            "error": str(e)
                        }
                    pbar.update(1)
        
        return results


def load_eval_data(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½è¯„æµ‹æ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


def save_results(results: List[Dict[str, Any]], output_path: str):
    """ä¿å­˜è¯„æµ‹ç»“æœ"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"âœ“ ç»“æœå·²ä¿å­˜: {output_path}")
