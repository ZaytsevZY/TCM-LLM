#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯„æµ‹å·¥å…·ç±»ï¼ˆæ”¯æŒå¹¶å‘ï¼‰
æ”¯æŒæœ¬åœ°LoRAæ¨¡å‹å’ŒAPIä¸¤ç§è¯„æµ‹æ–¹å¼
"""
import json
import time
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import openai
from tqdm import tqdm

class ModelEvaluator:
    """æ¨¡å‹è¯„æµ‹å™¨"""
    
    def __init__(self, mode="local", model_path=None, lora_path=None, api_config=None):
        """
        åˆå§‹åŒ–è¯„æµ‹å™¨
        
        Args:
            mode: "local" (æœ¬åœ°LoRAæ¨¡å‹) æˆ– "api" (APIè¯„æµ‹)
            model_path: åŸºåº§æ¨¡å‹è·¯å¾„
            lora_path: LoRAæ¨¡å‹è·¯å¾„
            api_config: APIé…ç½®å­—å…¸
        """
        self.mode = mode
        
        if mode == "local":
            print("ğŸ”§ åŠ è½½æœ¬åœ°LoRAæ¨¡å‹...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            # åŠ è½½åŸºåº§æ¨¡å‹
            self.base_model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # åŠ è½½LoRAæƒé‡
            self.model = PeftModel.from_pretrained(
                self.base_model,
                lora_path,
                torch_dtype=torch.bfloat16
            )
            self.model.eval()
            print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
            
        elif mode == "api":
            print("ğŸ”§ é…ç½®APIè¯„æµ‹...")
            self.client = openai.OpenAI(
                api_key=api_config["api_key"],
                base_url=api_config["base_url"]
            )
            self.api_model = api_config["model_name"]
            self.api_config = api_config
            print("âœ“ APIé…ç½®å®Œæˆ")
    
    def generate(self, prompt: str, max_tokens: int = 2048, temperature: float = 0.1) -> str:
        """
        ç”Ÿæˆå›ç­”
        
        Args:
            prompt: è¾“å…¥prompt
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if self.mode == "local":
            return self._generate_local(prompt, max_tokens, temperature)
        elif self.mode == "api":
            return self._generate_api(prompt, max_tokens, temperature)
    
    def _generate_local(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """æœ¬åœ°æ¨¡å‹ç”Ÿæˆ"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=temperature > 0,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # åªè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†
        generated_text = self.tokenizer.decode(
            outputs[0][len(inputs.input_ids[0]):],
            skip_special_tokens=True
        )
        return generated_text.strip()
    
    def _generate_api(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """APIç”Ÿæˆï¼ˆå¸¦é‡è¯•ï¼‰"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.api_model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    print(f"APIè°ƒç”¨å¤±è´¥: {e}")
                    return ""
    
    def _evaluate_single(
        self,
        item: Dict[str, Any],
        prompt_builder,
        max_tokens: int
    ) -> Dict[str, Any]:
        """
        è¯„æµ‹å•ä¸ªæ ·æœ¬
        
        Args:
            item: å•ä¸ªè¯„æµ‹æ ·æœ¬
            prompt_builder: promptæ„å»ºå‡½æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            è¯„æµ‹ç»“æœ
        """
        start_time = time.time()
        
        # æ„å»ºprompt
        prompt = prompt_builder(item["full_question"])
        
        # ç”Ÿæˆå›ç­”
        prediction = self.generate(prompt, max_tokens=max_tokens)
        
        inference_time = time.time() - start_time
        
        # è¿”å›ç»“æœ
        result = {
            "id": item["id"],
            "instruction": item["instruction"],
            "input": item["input"],
            "full_question": item["full_question"],
            "reference": item["output"],
            "prediction": prediction,
            "inference_time": inference_time
        }
        
        return result
    
    def batch_evaluate(
        self,
        eval_data: List[Dict[str, Any]],
        prompt_builder,
        mode_name: str,
        max_tokens: int = 2048,
        num_workers: int = 1
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡è¯„æµ‹ï¼ˆæ”¯æŒå¹¶å‘ï¼‰
        
        Args:
            eval_data: è¯„æµ‹æ•°æ®åˆ—è¡¨
            prompt_builder: promptæ„å»ºå‡½æ•°
            mode_name: è¯„æµ‹æ¨¡å¼åç§°ï¼ˆ"zero_shot"æˆ–"cot"ï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            num_workers: å¹¶å‘çº¿ç¨‹æ•°ï¼ˆä»…APIæ¨¡å¼æœ‰æ•ˆï¼‰
            
        Returns:
            è¯„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        
        print(f"\nğŸ”„ å¼€å§‹{mode_name}è¯„æµ‹ ({len(eval_data)}æ¡)...")
        print(f"å¹¶å‘æ•°: {num_workers}")
        
        if self.mode == "api" and num_workers > 1:
            # APIæ¨¡å¼ä½¿ç”¨å¹¶å‘
            results = self._batch_evaluate_parallel(
                eval_data, prompt_builder, mode_name, max_tokens, num_workers
            )
        else:
            # æœ¬åœ°æ¨¡å¼æˆ–å•çº¿ç¨‹
            results = self._batch_evaluate_sequential(
                eval_data, prompt_builder, mode_name, max_tokens
            )
        
        print(f"âœ“ {mode_name}è¯„æµ‹å®Œæˆ")
        return results
    
    def _batch_evaluate_sequential(
        self,
        eval_data: List[Dict[str, Any]],
        prompt_builder,
        mode_name: str,
        max_tokens: int
    ) -> List[Dict[str, Any]]:
        """ä¸²è¡Œè¯„æµ‹"""
        results = []
        for item in tqdm(eval_data, desc=f"{mode_name}è¯„æµ‹"):
            result = self._evaluate_single(item, prompt_builder, max_tokens)
            results.append(result)
        return results
    
    def _batch_evaluate_parallel(
        self,
        eval_data: List[Dict[str, Any]],
        prompt_builder,
        mode_name: str,
        max_tokens: int,
        num_workers: int
    ) -> List[Dict[str, Any]]:
        """å¹¶è¡Œè¯„æµ‹ï¼ˆAPIæ¨¡å¼ï¼‰"""
        results = [None] * len(eval_data)
        
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_idx = {
                executor.submit(
                    self._evaluate_single, 
                    item, 
                    prompt_builder, 
                    max_tokens
                ): idx
                for idx, item in enumerate(eval_data)
            }
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(total=len(eval_data), desc=f"{mode_name}è¯„æµ‹(å¹¶å‘)") as pbar:
                for future in as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    try:
                        result = future.result()
                        results[idx] = result
                    except Exception as e:
                        print(f"\næ ·æœ¬ {idx} è¯„æµ‹å¤±è´¥: {e}")
                        # åˆ›å»ºå¤±è´¥ç»“æœ
                        results[idx] = {
                            "id": eval_data[idx]["id"],
                            "prediction": "",
                            "inference_time": 0,
                            "error": str(e)
                        }
                    pbar.update(1)
        
        return results


def load_eval_data(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½è¯„æµ‹æ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


def save_results(results: List[Dict[str, Any]], output_path: str):
    """ä¿å­˜è¯„æµ‹ç»“æœ"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"âœ“ ç»“æœå·²ä¿å­˜: {output_path}")
